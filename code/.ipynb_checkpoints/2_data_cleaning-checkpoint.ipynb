{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this segment, we will be looking at some fundamental types of data cleaning, and how we can leverage what we've learned so far to help us get data we can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common issues we find in working with real world data is dealing with missing values in our data set.  A missing value is any value that denotes that the data point is not recorded.  In Python, these values are often empty values for different data types, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None # python's null value\n",
    "np.nan # not a number value from numpy\n",
    "np.inf # infinite from numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pandas, the majority of missing values will come in the form of `np.nan`.  Because `np.nan` is literally not a number, any operation involving `np.nan` will also result in `np.nan`, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, np.nan, 3, 4]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, np.nan, 3, 4]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`None` will behave differently - most operations involving None will result in an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, None, 3, 4]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we deal with Nones far less in Pandas because as long as we're dealing with numeric data, `None`s are generally cast into `np.nan`s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([None], dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, `np.inf` behaves very similarly to `np.nan` since most operations with infinity will result in infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, np.inf, 3, 4]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf / np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load a data set, we'll need to check for `np.nan`, `np.inf` and in some cases `None` before we do any analysis.  Luckily `numpy` has a few functions to help us with `np.isnan`, `np.isinf` and `np.isfinite` which checks for both, and pandas gives us a helpful function `pd.isnull` which covers `None` cases as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan([1, np.nan, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isinf([1, np.inf, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isfinite([1, np.inf, np.nan, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull([np.nan, np.inf, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see if there are any missing values by using the `any` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull([np.nan, np.inf, None]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull([1, 2, None]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull([1, 2, 3]).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering & Dropping missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these functions to help us filter our Series or DataFrames.  For example, if we are looking to filter out all the values that are nan, we can do so easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps = pd.Series([954.7, 514.4, np.nan, None, 57.9, 45.7, np.inf, 38.7, 28.8, 25.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note**: `pd.Series.sum()` will ignore `np.nan`, but _not_ `np.inf` as we see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps[~np.isnan(market_caps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps[np.isfinite(market_caps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps[np.isfinite(market_caps)].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the above functions will apply to DataFrames as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, np.nan],\n",
    "    'B': [4, 5, None, np.nan],\n",
    "    'C': [7, np.nan, np.nan, None]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isfinite(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.isnull(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this also means that we can get a count of nulls by column and by row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(sample_df).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(sample_df).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this, we can filter on any column.  For example, if we want to filter out anything with null from column A, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[sample_df.A.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this removed the 2nd row (we can see this since index `1` is now gone).  \n",
    "\n",
    "Because of how common this operation is, pandas has a method to do this in one operation using `.dropna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.dropna(subset=['A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by default `.dropna` will drop a row if _any_ element in that row has a null value.  we can use `subset` to select which columns we care about for dropping.  E.g. if we just called `.dropna` without any arguments on our DataFrame, we would get back just the first row since it's the only row that's non-null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lastly, we can drop by columns also (i.e. if any column has a null, we drop it).  In our case this would only leave no columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also tune how aggressively dropna drops.  The `how` argument allows us to specify drop on `any` nulls or `all` nulls, and the `thresh` argument allows us to specify how many nulls need to occur before dropping the row/column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.dropna(axis=1, thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we don't want to lose data, and replacing missing values with something else could be more advantageous than dropping entire rows or columns (which are the only two choices when working with tabular data).  For example, this may be because the columns with missing data are low importance (and thus dropping high valued rows due to low importance features could be bad), or because we have columns that are designed to detect outliers, and so it is safe to fill in missing data with the median value versus dropping data points.\n",
    "\n",
    "To do this we can use the `.fillna` method that's on both Series and DataFrames, which allows us to fill missing data points with a variety of different strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to fill in missing values is to use a single value to fill in, e.g. we can fill in with a 0, or with the sample mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps = pd.Series([954.7, 514.4, np.nan, None, 57.9, 45.7, np.nan, 38.7, 28.8, 25.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps.fillna(market_caps.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to fill specific items but not others, this can also done by passing in a Series, which will fill by its index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps.fillna(pd.Series({3:100}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one really handy feature is the ability to forwardfill or backfill.  Forwardfill will fill the previously non-null value to the current null position recursively, and backfill will do the opposite.\n",
    "\n",
    "This is particularly useful for time series, especially time series that are not in regular intervals, since it allows you to create a complete data set by filling down every timestamp from your joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_caps.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframes have the same method, but can work on both dimensions (like most DataFrame functions), e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.fillna({'A': 4, 'B': sample_df.B.mean()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.assign(C=sample_df['C'].fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second most common problem you will likely encounter is the need to replace values.  This could be due to outliers (e.g. your data set reports 120% for an feature than can only be 100% maximum), mangled ingestion (e.g. all values that should say \"BTC\" say \"BTT\"), or you need the values to be formatted in a certain way before using the data (e.g. all values that say \"Ethereum\" needs to say \"ETH\").  \n",
    "\n",
    "In this situation we will still need to take action similar to what we did for missing data, but `.isnull`, `.dropna` and `.fillna` will no longer help us since these values will not be considered nulls.\n",
    "\n",
    "This problem becomes even more difficult when we have large data sets, and the outliers can be small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'pct_scored': np.random.permutation(\n",
    "        np.append(\n",
    "            100 * np.random.rand(980),\n",
    "            100 + np.random.rand(20)\n",
    "        )\n",
    "    ),\n",
    "    'district': np.random.permutation(\n",
    "        np.append(\n",
    "            np.random.choice(['West', 'South', 'East', 'North'], 990), \n",
    "            (np.random.choice(['Nt', 'Sth'], 10))\n",
    "        )\n",
    "    ),\n",
    "    'city': np.random.permutation(\n",
    "        np.append(\n",
    "            np.random.choice(['Westbrook', 'Southbridge', 'Eastriver', 'Northgate'], 990), \n",
    "            (np.random.choice(['Westnook', 'Northgrate'], 10))\n",
    "        )\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Identifying categorical values that need replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical values, one of the easiest ways to find whether we have values that need replacing is to identify the distinct set of values and see if there's anything strange.  For example, imagine we had a data set like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can identify the unique set of names for district using `.unique`.  From this we found that there's two values we didn't expect `Sth` and `Nt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['district'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not be clear what values are actually errors.  We can use `.value_counts` to spot outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['district'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this shows us that Nt and Sth are likely errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we know that Sth is shorthand for South, and Nt is shorthand for North (this error could be due to the source data being manual entry).  We can use `.replace` to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['district'].replace('Nt', 'North').value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However if we have multiple values to replace, this is pretty inefficient.  Instead, we can pass in a dict and replace everything at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['district'].replace({'Nt': 'North', 'Sth': 'South'}).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a DataFrame, we can do this across multiple columns as well with a nested Dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({\n",
    "    'district': {'Nt': 'North', 'Sth': 'South'},\n",
    "    'city': {'Westnook': 'Westbrook', 'Northgrate': 'Northgate'}\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace({\n",
    "    'district': {'Nt': 'North', 'Sth': 'South'},\n",
    "    'city': {'Westnook': 'Westbrook', 'Northgrate': 'Northgate'}\n",
    "})['city'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying numerical values that need replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out how we can easily replace categorical values, but what about numerical outliers?  This is even easier.  We can first identify how many row we will need to replace, then replace it using DataFrame mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.pct_scored > 100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.pct_scored > 100, 'pct_scored'] = 100\n",
    "df.loc[df.pct_scored > 100].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Duplicate Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate values are super simple to detect and resolve - the standard methodology is to remove all but one of the duplicated values, or remove all duplicates (as it may indicate something wrong with the upstream data).  We can use `.duplicated` to detect duplicates, and `.drop_duplicates` to remove them.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'A': ['A', 'B', 'C', 'B', 'D', 'D', 'D'],\n",
    "    'B': [1, 2, 3, 4, 5, 5, 7]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently, duplicated shows that only one row is duplicated.  This is the case because by default duplicated will check for fully duplicated rows - i.e. _every_ element in the row is duplicated.  If we want to only check for duplicates in column A, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that by default only the occurrence beyond the first is considered a duplicate (i.e. the first occurrence sequentially is not a duplicate).  We can change the behavior by either choosing the last occurrence as non-duplicated, or no occurrences as non-duplicated by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['A'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['A'], keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know what items are duplicated and our strategy for keeping records, we can simply call `.drop_duplicates` with the exact same parameters to return a DataFrame with only our unique rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['A'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Badly formatted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Last case we will go over is handling malformed text.  This can happen for example if you're ingesting data from an HTML source where markup is attached to the data, or if the data is formatted in a weird or unintuitive way.  \n",
    "\n",
    "Luckily, pandas gives us some pretty easy methods to be able to split, truncate and parse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a data set like below, we can do the \"text to column\" operation using `.str.split`, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'records': [\n",
    "        '<b>A</b>_59_D    @',\n",
    "        '<b>B</b>_92_L    L',\n",
    "        '<b>C</b>_7_O  Q',\n",
    "        '<b>D</b>_43_V  O',\n",
    "        '<b>E</b>_50_J    C',\n",
    "        '<b>F</b>_53_@   U',\n",
    "        '<b>G</b>_17_K  C',\n",
    "        '<b>H</b>_24_K  T',\n",
    "        '<b>I</b>_58_K    T',\n",
    "        '<b>J</b>_94_L M',\n",
    "        '<b>K</b>_60_H M',\n",
    "        '<b>L</b>_65_Q E',\n",
    "        '<b>M</b>_23_A C',\n",
    "        '<b>N</b>_62_PS',\n",
    "        '<b>O</b>_90_P    X',\n",
    "        '<b>P</b>_34_O    D',\n",
    "        '<b>Q</b>_26_T  D',\n",
    "        '<b>R</b>_78_P   T',\n",
    "        '<b>S</b>_94_@   C',\n",
    "        '<b>T</b>_69_?  E',\n",
    "        '<b>U</b>_50_P T',\n",
    "        '<b>V</b>_99_T  C',\n",
    "        '<b>W</b>_20_V Q',\n",
    "        '<b>X</b>_88_E    O',\n",
    "        '<b>Y</b>_7_RF',\n",
    "        '<b>Z</b>_47_EN',\n",
    "    ]\n",
    "})\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = df.records.str.split('_', expand=True)\n",
    "df_res.columns = ['column_A', 'column_B', 'column_C']\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can strip out end whitespaces from Column C, and strip out middle whitespaces using replace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['column_C'] = df_res.column_C.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['column_C'] = df_res.column_C.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lastly, the extract (and replace function above) function is very, very powerful in that it can use any regex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['column_A'] = df_res['column_A'].str.extract('<b>(.|\\n)*?<\\/b>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
