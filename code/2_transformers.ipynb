{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc45256",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0bec4",
   "metadata": {},
   "source": [
    "In this model we'll see a few common transformations that we use when we pre-process data before training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fb980",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f26cd50",
   "metadata": {},
   "source": [
    "One of the most common steps that we will need to do before running a model is pre-processing our features.  This may be as simple as standardizing your features so that they are the same scale, all the way to mapping your empirical data to a guassian distribution.  `sklearn` has a suite of built-in preprocessors to help us do this easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356f34e",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40962a",
   "metadata": {},
   "source": [
    "Standardization is simply taking a set of data points, subtracting out the mean and dividing by its standard deviation.\n",
    "\n",
    "It is often needed for most machine learning models, as features with different scale and means can dramatically affect the estimated results.  It's often good practice to standardize features by default, and only not standardize if there's a very good reason to do so.\n",
    "\n",
    "To standardize, we can use sklearn's transformers to help us.  For example, if we want to standardize a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c013103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb351fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([\n",
    "    [ 1., -1.,  2.],\n",
    "    [ 2.,  0.,  0.],\n",
    "    [ 0.,  1., -1.]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d157dab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa16a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.        , 0.33333333])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4b14e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81649658, 0.81649658, 1.24721913])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63dbf646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = scaler.transform(X_train)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29147d7",
   "metadata": {},
   "source": [
    "**note**: we can do both steps by calling `fit_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2da944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X_train)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5e43b",
   "metadata": {},
   "source": [
    "we can now verify that we have standardized the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1cea7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6220b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bfbca0",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86020c2f",
   "metadata": {},
   "source": [
    "A more generalized version of standardization is normalization, where we scale the data to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f2e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed101cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405a6983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.70710678, -0.70710678]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ed623",
   "metadata": {},
   "source": [
    "you can also use other norms, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8851ec5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25, -0.25,  0.5 ],\n",
       "       [ 1.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.5 , -0.5 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer_l1 = Normalizer(norm='l1')\n",
    "normalizer_l1.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61132a",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77329de7",
   "metadata": {},
   "source": [
    "There will be certain situations where we prefer to scale our features rather than standardize them.  We may want to do this for data sets with a lot of zeros, where zeros are meaningful.\n",
    "\n",
    "We can scale our input matrix above to `[-1, 1]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88304815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f31fbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1c2671a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2f4deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840fca3",
   "metadata": {},
   "source": [
    "We can see in the above example that we have scaled our dataset to `[-1, 1]`, however we have also protected zero entries.\n",
    "\n",
    "If we want to scale our data to some arbitrary `[a, b]`, then we can use `MinMaxScaler` instead, which works the same way as `MaxAbsScaler`, but it can be initialized with a `feature_range=(min, max)` to specific the range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883d592",
   "metadata": {},
   "source": [
    "### Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8fce55",
   "metadata": {},
   "source": [
    "Sometimes we can generate higher signals from our features by grouping our data in a logical way.  Quantile grouping is one very common way to transform features - in this method we take our data points and map them to a uniform (or normal) distribution.  This has two major effects:\n",
    "- it spreads out the data when data is tightly clustered, and groups data that is sparse\n",
    "- it reduces the impact of outliers, since they will just be grouped into the top or bottom quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee05b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7107e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4de2ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.83333333, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.44444444, 0.11111111, 0.11111111],\n",
       "       [0.07407407, 0.66666667, 0.08333333, 0.11111111],\n",
       "       [0.05555556, 0.57575758, 0.22222222, 0.11111111],\n",
       "       [0.16666667, 0.88888889, 0.11111111, 0.11111111],\n",
       "       [0.33333333, 0.93055556, 0.24183007, 0.25423729],\n",
       "       [0.05555556, 0.77777778, 0.11111111, 0.23728814],\n",
       "       [0.16666667, 0.77777778, 0.22222222, 0.11111111],\n",
       "       [0.01851852, 0.33333333, 0.11111111, 0.11111111],\n",
       "       [0.11111111, 0.57575758, 0.22222222, 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile = QuantileTransformer(random_state=0, n_quantiles=10)\n",
    "x_quantiles = quantile.fit_transform(X)\n",
    "x_quantiles[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3e611",
   "metadata": {},
   "source": [
    "after processing the data into quantiles, we can now see that the data is between `[0, 1]`, and with a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9d5b158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.3, 5.1, 5.8, 6.4, 7.9])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(X[:, 0], [0, 25, 50, 75, 100]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "582b6235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.22222222, 0.48444444, 0.72222222, 1.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(x_quantiles[:, 0], [0, 25, 50, 75, 100]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a6086",
   "metadata": {},
   "source": [
    "### Categorical Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8426f",
   "metadata": {},
   "source": [
    "Sometimes, our data isn't numeric but categorical, however for most machine learning models, non-numeric inputs tend to be fairly difficult to deal with.  As a result we will need to encode our categorical variables into numeric equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3ce83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3667109",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "    ['Berkeley', 'Male', 'Masters'],\n",
    "    ['Oakland', 'Male', 'Bachelors'],\n",
    "    ['Berkeley', 'Female', 'PhD']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cac90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0c2e315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 0., 2.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06b9bc",
   "metadata": {},
   "source": [
    "we can see from the above that we have converted our categorical features to ordinal features.  However, this is not always useful for modeling since models will take these variables as numeric.  As an example, we cannot take this transformed data and use it for a regression.  \n",
    "\n",
    "Instead, we can use `one hot encoding` (aka dummy variables) to turn our categorical features to a set of dummy features that we can now use in most downstream models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85bd6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed6dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae67356d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1., 0.],\n",
       "       [0., 1., 0., 1., 1., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13642431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Berkeley', 'Oakland'], dtype=object),\n",
       " array(['Female', 'Male'], dtype=object),\n",
       " array(['Bachelors', 'Masters', 'PhD'], dtype=object)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7b59e",
   "metadata": {},
   "source": [
    "**note** this showcases another really powerful usecase of `Transfomers` - the resulting output does not need to be the same number of columns as the input.  In our case we have 7 distinct categorical variables in 3 rows, and we get a 3x7 matrix as a result.\n",
    "\n",
    "We can also pre-specify the categories - this especially useful if the data set doesn't include all possible categories, but it is important for the model to incorporate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2905432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot = OneHotEncoder(categories=[\n",
    "    ['Berkeley', 'Oakland', 'San Francisco'],\n",
    "    ['Female', 'Male'],\n",
    "    ['Bachelors', 'Masters', 'PhD', 'High School']\n",
    "])\n",
    "hot.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3d5eb",
   "metadata": {},
   "source": [
    "we can now see that we have 9 categories represented in the data, even though we don't have 2 of them in our sample data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21a20d",
   "metadata": {},
   "source": [
    "However, if we were to run a regression, this output still would not work as the matrix is perfectly collinear.  Instead, we can simply add the `drop` argument to get us a noncollinear matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4cf00f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot = OneHotEncoder(drop='first') # can also use 'if_binary'\n",
    "hot.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d08eb",
   "metadata": {},
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f196fa",
   "metadata": {},
   "source": [
    "Discretization is useful when we don't need the granularity of continuous variables, or when we get higher signal:noise from the discrete/binned representation than the continous one.  \n",
    "\n",
    "One example of this can be for threshold signals (e.g. a binary option, where payout is 0 if stock price is under $100, and 100 otherwise) - if we wanted to regress payout vs stock price, having the discretized representation of the feature will yield a much better model than unsing the continous variable.\n",
    "\n",
    "To do this, we can use the `KBinsDiscretizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5ade6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f79863c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "923f51ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [2., 1., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab5153",
   "metadata": {},
   "source": [
    "we can transform the output of the discretization either as ordinal (above), or one-hot (below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d39de67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 1., 0., 1.],\n",
       "       [1., 0., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = KBinsDiscretizer(n_bins=[3, 2, 2])\n",
    "encoder.fit_transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de5b78",
   "metadata": {},
   "source": [
    "we can also change the way that `KBinsDiscretizer` cuts.  By default the transformer cuts using quantiles, however we can also do uniform cuts by setting `strategy='uniform'`, which will take the range and cut into even chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02becfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 1., 1., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = KBinsDiscretizer(strategy='uniform', n_bins=[3, 2, 2])\n",
    "encoder.fit_transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee754f9",
   "metadata": {},
   "source": [
    "Lastly, we can discretize to binary using the Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee4d8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67dca1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Binarizer(threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7dc2219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31222b0a",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0975f",
   "metadata": {},
   "source": [
    "We previously looked at how to deal with missing data as a part of data cleaning, and one method we mentioned was imputation.  Once we move from data analysis to modeling, we will need to build our imputation strategy into our modeling pipeline to make sure our training/testing process is consistent.  To do this, we can leverage `Transformers` again to help us tranform the data (in this case the tranformation is an imputation).\n",
    "\n",
    "The simplest way to impute is using the `SimpleImputer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37a5f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    [1, 2, 3],\n",
    "    [4, np.nan, 6],\n",
    "    [np.nan, np.nan, 9],\n",
    "    [1, 3, 7],\n",
    "    [6, 8, 1]\n",
    "]\n",
    "\n",
    "X_train_cat = [\n",
    "    ['a', '1'],\n",
    "    ['a', '2'],\n",
    "    [np.nan, '2'],\n",
    "    ['b', np.nan]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04885e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e297e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16b352",
   "metadata": {},
   "source": [
    "We can do the same type of imputation with categorial variables also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit_transform(X_train_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5340e5",
   "metadata": {},
   "source": [
    "In situations where there are strong relations between features, we can leverage multivariate imputers instead of having to rely on single-feature properties.  One common way to do this is via KNN-based imputation, i.e.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2914736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db572a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69de8d",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65413a91",
   "metadata": {},
   "source": [
    "Lastly, when we have a lot of features, we may want to reduce the dimensionality of the data before training the model on it.  One very popular way to do this is via PCA, which is at a very abstract level just another transformation on the data.\n",
    "\n",
    "We can use the PCA transformer to bring our higher dimensionality data to lower dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([\n",
    "    [-1, -1, 3], \n",
    "    [-2, -1, 10], \n",
    "    [-3, -2, 13], \n",
    "    [1, 1, 15], \n",
    "    [2, 1, 22], \n",
    "    [3, 2, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694816c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4a8ff",
   "metadata": {},
   "source": [
    "after transforming the data, we can now take the lower dimensionality inputs and use them to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211475e",
   "metadata": {},
   "source": [
    "## Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9de9d5",
   "metadata": {},
   "source": [
    "We've gone through many different feature transformation use cases, however there is a likelihood that none of the above will suit your specific use case.  In that situation, you can simply create your own Transformer.  Most transformers just need to inherit `BaseEstimator` (since all transformers are estimators), and the `TransformerMixin` which gives the transformer the `fit_transform` method.\n",
    "\n",
    "For example, we can create a Transformer that transforms a feature into a boolean column that is `true` if the value is not null and `false` otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e260bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088fc9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNullTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        #fit needs to be implemented as this is an estimator, however we don't need to fit anything\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return ~np.isnan(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fa7a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [-1, np.nan, 3], \n",
    "    [-2, -1, 10], \n",
    "    [-3, -2, 13], \n",
    "    [1, 1, np.nan], \n",
    "    [2, 1, 22], \n",
    "    [np.nan, 2, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac20b3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = BinaryNullTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1d7c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True, False],\n",
       "       [ True,  True,  True],\n",
       "       [False,  True,  True]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45333cb2-ec1e-4c88-ab8f-a38b27739f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
