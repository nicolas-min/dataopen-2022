color="black", fill="lightblue",  size = 1, alpha = .3) +
geom_point(data=MainCities[MainCities$country.etc == "CA",], aes(x=long, y=lat, size = pop/1000000), color = "gold", alpha = .5) +
theme_minimal() +
ggtitle("California Population")
#================
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
ggplot() +
geom_polygon( data=AllCounty, aes(x=long, y=lat, group=group),
color="darkblue", fill="lightblue", size = .1 ) +
geom_polygon( data=MainStates, aes(x=long, y=lat, group=group),
color="black", fill="darkblue",  size = 1, alpha = .3)+
geom_point(data=MainCities, aes(x=long, y=lat, size = pop/1000000), color = "gold", alpha = .5)+
theme_minimal()+
ggtitle("US Population")
ggplot() +
geom_polygon( data=AllCounty, aes(x=long, y=lat, group=group),
color="white", fill="lightblue", size = .1 ) +
geom_polygon( data=MainStates, aes(x=long, y=lat, group=group),
color="black", fill="darkblue",  size = 1, alpha = .3)+
geom_point(data=MainCities, aes(x=long, y=lat, size = pop/1000000), color = "gold", alpha = .5)+
theme_minimal()+
ggtitle("US Population")
ggsave("../plot/sample plot.png")
library(here)
#read datasets in R
MainStates <- map_data("state") #state location (grouped)
library(maps)
#read datasets in R
MainStates <- map_data("state") #state location (grouped)
library(dplyr)
#read datasets in R
MainStates <- map_data("state") #state location (grouped)
library(ggplot2)
#read datasets in R
MainStates <- map_data("state") #state location (grouped)
AllCounty <- map_data("county") #county location (grouped)
MainCities <- filter(us.cities, long>=-130) #population by city
# Dummy data
x <- LETTERS[1:20]
x
y <- paste0("var", seq(1,20))
y
data <- expand.grid(X=x, Y=y)
data
View(data)
data$Z <- runif(400, 0, 5)
# Heatmap
ggplot(data, aes(X, Y, fill= Z)) +
geom_tile()
# Heatmap
ggplot(data, aes(X, Y, fill= Z)) +
geom_tile() +
scale_fill_distiller(palette = "RdPu") +
theme_ipsum()
# Heatmap
ggplot(data, aes(X, Y, fill= Z)) +
geom_tile() +
scale_fill_distiller(palette = "RdPu") +
theme_ipsum()
# Heatmap
ggplot(data, aes(X, Y, fill= Z)) +
geom_tile() +
scale_fill_distiller(palette = "RdPu")
#https://r-graph-gallery.com/79-levelplot-with-ggplot2.html
library(hrbrthemes)
install.packages("hrbrthemes")
#https://r-graph-gallery.com/79-levelplot-with-ggplot2.html
library(hrbrthemes)
# Heatmap
ggplot(data, aes(X, Y, fill= Z)) +
geom_tile() +
scale_fill_distiller(palette = "RdPu") +
theme_ipsum()
# Heatmap
ggplot(data, aes(X, Y, fill= Z)) +
geom_tile() +
scale_fill_distiller(palette = "RdPu")
# Heatmap
ggplot(data, aes(X, Y, fill= Z)) +
geom_tile() +
scale_fill_distiller(palette = "PiYG") +
theme_ipsum()
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
Gov_mortage <- fread("Government_Mortage.csv")
#ML
# Installing the package
install.packages("dplyr")
#ML
install.packages("data.table")
# Loading package
library(data.table)
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
Gov_mortage <- fread("Government_Mortage.csv")
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
data(BostonHousing)
data(BostonHousing2)
library(mlbench)
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
data(BostonHousing)
data(BostonHousing2)
# Loading data
data(iris)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.7)
# Loading package
library(e1071)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.7)
library(caTools)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.7)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.7)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.1)
# Splitting data into train
# and test data
split <- sample.split(BostonHousing2, SplitRatio = 0.1)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.1)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.8)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.8)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.85)
split
split == "TRUE"
?subset
View(train_cl)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.99)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.99)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.8)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.9)
# Splitting data into train
# and test data
split <- sample.split(iris, SplitRatio = 0.9)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
?sample.split
# Splitting data into train
# and test data
split1<- sample(c(rep(0, 0.7 * nrow(iris)), rep(1, 0.3 * nrow(iris))))
rep(0, 0.7 * nrow(iris)
)
0.7 * nrow(iris)
nrow(iris)
0.72 * nrow(iris)
0.73 * nrow(iris)
# Splitting data into train
# and test data
split1<- sample(c(rep(0, 0.73 * nrow(iris)), rep(1, 0.27 * nrow(iris))))
0.73 * nrow(iris)
rep(0, 0.73 * nrow(iris))
0.73 * nrow(iris)
length(rep(0, 0.73 * nrow(iris)))
# Splitting data into train
# and test data
split1<- sample(c(rep(0, 0.73 * nrow(iris)), rep(1, 0.27 * nrow(iris))))
split1
train_cl <- iris[split1 == 0]
train_cl <- iris[split1 == 0,]
test_cl <- iris[split1 == 1,]
# Splitting data into train
# and test data
training_percent <- 0.78
# Splitting data into train
# and test data
training_percent <- 0.78
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train_cl <- iris[split1 == 0,]
test_cl <- iris[split1 == 1,]
# Splitting data into train
# and test data
training_percent <- 0.99
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train_cl <- iris[split1 == 0,]
test_cl <- iris[split1 == 1,]
# Splitting data into train
# and test data
training_percent <- 0.75
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train_cl <- iris[split1 == 0,]
test_cl <- iris[split1 == 1,]
# Feature Scaling
train_scale <- scale(train_cl[, 1:4])
View(train_scale)
mean(train_scale)
?scale
View(train_cl)
# Feature Scaling
train_scale <- scale(train_cl$Sepal.Length)
View(train_scale)
mean(train_scale)
# Feature Scaling
train_scale <- scale(train_cl[, 1:4])
test_scale <- scale(test_cl[, 1:4])
# Fitting Naive Bayes Model
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,data = train_cl)
classifier_cl
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test_cl)
# Confusion Matrix
cm <- table(test_cl$Species, y_pred)
cm
y_pred
# Confusion Matrix
cm <- table(test_cl$Species, y_pred)
cm
y_pred==test_cl$Species
sum(y_pred==test_cl$Species)/length(test_cl$Species)
# For implementing random forest algorithm
install.packages("randomForest")
urlPackage <- "https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-12.tar.gz"
install.packages(urlPackage, repos=NULL, type="source")
# For implementing random forest algorithm
library("randomForest")
classifier_cl <- naiveBayes(Species ~ .,data = train_cl)
classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)
classifier_RF <- randomForest(x = train_cl[-5], y = train_cl$Species, ntree = 500)
classifier_RF
classifier_cl
y_pred = predict(classifier_RF, newdata = test[-5])
classifier_RF <- randomForest(x = train_cl[-5], y = train_cl$Species, ntree = 500)
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
data(BostonHousing)
data(BostonHousing2)
data(iris)
# Loading package
library(e1071)
library(caTools)
library(caret)
library(mlbench)
# Splitting data into train
# and test data
training_percent <- 0.75
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train <- iris[split1 == 0,]
test <- iris[split1 == 1,]
# Feature Scaling
train_scale <- scale(train[, 1:4])
test_scale <- scale(test[, 1:4])
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,data = train)
classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)
# Predicting on test data
y_pred_cl <- predict(classifier_cl, newdata = test)
y_pred_RF = predict(classifier_RF, newdata = test[-5])
# Confusion Matrix
cm_cl <- table(test_cl$Species, y_pred_cl)
cm_cl
cm_RF <- table(test_cl$Species, y_pred_RF)
cm_RF
#accuracy
sum(y_pred_cl==test_cl$Species)/length(test_cl$Species)
sum(y_pred_RF==test_cl$Species)/length(test_cl$Species)
# Splitting data into train
# and test data
training_percent <- 0.75
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
split1
train <- iris[split1 == 0,]
test <- iris[split1 == 1,]
# Feature Scaling
train_scale <- scale(train[, 1:4])
test_scale <- scale(test[, 1:4])
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,data = train)
# Predicting on test data
y_pred_cl <- predict(classifier_cl, newdata = test)
# Confusion Matrix
cm_cl <- table(test_cl$Species, y_pred_cl)
cm_cl
classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)
y_pred_RF = predict(classifier_RF, newdata = test[-5])
cm_RF <- table(test_cl$Species, y_pred_RF)
cm_RF
sum(y_pred_RF==test_cl$Species)/length(test_cl$Species)
#accuracy
sum(y_pred_cl==test_cl$Species)/length(test_cl$Species)
cm_cl
cm_RF
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
data(BostonHousing)
data(BostonHousing2)
data(iris)
# Loading package
library(e1071)
library(caTools)
library(caret)
library(mlbench)
# Splitting data into train
# and test data
training_percent <- 0.75
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train <- iris[split1 == 0,]
test <- iris[split1 == 1,]
# Feature Scaling
train_scale <- scale(train[, 1:4])
test_scale <- scale(test[, 1:4])
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,data = train)
classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)
# Predicting on test data
y_pred_cl <- predict(classifier_cl, newdata = test)
y_pred_RF = predict(classifier_RF, newdata = test[-5])
# Confusion Matrix
cm_cl <- table(test_cl$Species, y_pred_cl)
cm_cl
cm_RF <- table(test_cl$Species, y_pred_RF)
cm_RF
#accuracy
sum(y_pred_cl==test_cl$Species)/length(test_cl$Species)
sum(y_pred_RF==test_cl$Species)/length(test_cl$Species)
# Splitting data into train
# and test data
training_percent <- 0.75
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train <- iris[split1 == 0,]
test <- iris[split1 == 1,]
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,data = train)
classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)
# Predicting on test data
y_pred_cl <- predict(classifier_cl, newdata = test)
y_pred_RF = predict(classifier_RF, newdata = test[-5])
# Confusion Matrix
cm_cl <- table(test_cl$Species, y_pred_cl)
# Confusion Matrix
cm_cl <- table(test$Species, y_pred_cl)
cm_cl
cm_RF <- table(test$Species, y_pred_RF)
cm_RF
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
data(BostonHousing)
data(BostonHousing2)
data(iris)
# Loading package
library(e1071)
library(caTools)
library(caret)
library(mlbench)
# Splitting data into train
# and test data
training_percent <- 0.75
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train <- iris[split1 == 0,]
test <- iris[split1 == 1,]
# Feature Scaling
train_scale <- scale(train[, 1:4])
test_scale <- scale(test[, 1:4])
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,data = train)
classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)
# Predicting on test data
y_pred_cl <- predict(classifier_cl, newdata = test)
y_pred_RF = predict(classifier_RF, newdata = test[-5])
# Confusion Matrix
cm_cl <- table(test$Species, y_pred_cl)
cm_cl
cm_RF <- table(test$Species, y_pred_RF)
cm_RF
#accuracy
sum(y_pred_cl==test_cl$Species)/length(test_cl$Species)
sum(y_pred_RF==test_cl$Species)/length(test_cl$Species)
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
data(BostonHousing)
data(BostonHousing2)
data(iris)
# Loading package
library(e1071)
library(caTools)
library(caret)
library(mlbench)
# Splitting data into train
# and test data
training_percent <- 0.75
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train <- iris[split1 == 0,]
test <- iris[split1 == 1,]
# Feature Scaling
train_scale <- scale(train[, 1:4])
test_scale <- scale(test[, 1:4])
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,data = train)
classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)
# Predicting on test data
y_pred_cl <- predict(classifier_cl, newdata = test)
y_pred_RF = predict(classifier_RF, newdata = test[-5])
# Confusion Matrix
cm_cl <- table(test$Species, y_pred_cl)
cm_cl
cm_RF <- table(test$Species, y_pred_RF)
cm_RF
#accuracy
sum(y_pred_cl==test$Species)/length(test_cl$Species)
sum(y_pred_RF==test$Species)/length(test_cl$Species)
#ML
#https://www.geeksforgeeks.org/7-best-r-packages-for-machine-learning/
data(BostonHousing)
data(BostonHousing2)
data(iris)
# Loading package
library(e1071)
library(caTools)
library(caret)
library(mlbench)
# Splitting data into train
# and test data
training_percent <- 0.75
testing_percent <- 1- training_percent
split1<- sample(c(rep(0, training_percent * nrow(iris)), rep(1, testing_percent * nrow(iris))))
train <- iris[split1 == 0,]
test <- iris[split1 == 1,]
# Feature Scaling
train_scale <- scale(train[, 1:4])
test_scale <- scale(test[, 1:4])
# to training dataset
set.seed(120) # Setting Seed
classifier_cl <- naiveBayes(Species ~ .,data = train)
classifier_RF <- randomForest(x = train[-5], y = train$Species, ntree = 500)
# Predicting on test data
y_pred_cl <- predict(classifier_cl, newdata = test)
y_pred_RF = predict(classifier_RF, newdata = test[-5])
# Confusion Matrix
cm_cl <- table(test$Species, y_pred_cl)
cm_cl
cm_RF <- table(test$Species, y_pred_RF)
cm_RF
#accuracy
sum(y_pred_cl==test$Species)/length(test$Species)
sum(y_pred_RF==test$Species)/length(test$Species)
train[5]
##run knn function
classifier_knn <- knn(train[1:4],test[1:4],cl=train[5],k=13)
##load the package class
library(class)
##run knn function
classifier_knn <- knn(train[1:4],test[1:4],cl=train[5],k=13)
train[1:4]
##run knn function
classifier_knn <- knn(train[1:4],test[1:4],cl=train[5],k=13)
##run knn function
classifier_knn <- knn(train,test,cl=train[5],k=13)
##run knn function
classifier_knn <- knn(train[1:4],test[1:4],cl=train[,5],k=13)
train[,5]
classifier_knn
##run knn function
y_pred_knn <- knn(train[1:4],test[1:4],cl=train[,5],k=13)
cm_knn <- table(test$Species,y_pred_knn)
cm_knn
sum(y_pred_knn==test$Species)/length(test$Species)
y_pred_knn <- knn(train[1:4],test[1:4],cl=train[,5],k=1) #no need to 'train'
cm_knn <- table(test$Species,y_pred_knn)
cm_knn
y_pred_knn <- knn(train[1:4],test[1:4],cl=train[,5],k=2) #no need to 'train'
cm_knn <- table(test$Species,y_pred_knn)
cm_knn
